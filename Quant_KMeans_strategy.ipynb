{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMPm9iDg5spEi90JI18Qld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JianfengMI/MLprojects/blob/main/Quant_KMeans_strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Import libriries and globle parameters"
      ],
      "metadata": {
        "id": "pHHEc1O3tBqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7nFY7NHsklU"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "import statsmodels.api as sm\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "RNG = 42\n",
        "np.random.seed(RNG)\n",
        "\n",
        "# Parameters\n",
        "START_DATE = (datetime.today() - timedelta(days=365*6)).strftime(\"%Y-%m-%d\")  # 6y of history\n",
        "END_DATE = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "BENCH = \"SPY\"\n",
        "MIN_PRICE_HISTORY_DAYS = 252  # at least 1 year\n",
        "KS = range(2, 11)  # k values to test\n",
        "REBALANCE_FREQ = \"M\"  # monthly rebalancing (pandas offset alias)\n",
        "LOOKBACK_YEARS = 3  # how many years of history for features (used when grabbing prices)\n",
        "MAX_WEIGHT = 0.10  # cap individual stock weight after momentum weighting (optional)\n",
        "TRANSACTION_COST_BPS = 5  # basis points per turnover (0.0005 per $1) - set 0 to ignore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download SP500 tickers and download the prices"
      ],
      "metadata": {
        "id": "zwr-z9qHtzZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp500_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status() # Raise an exception for HTTP errors\n",
        "\n",
        "    # pandas can directly parse tables from HTML content\n",
        "    tables = pd.read_html(response.text)\n",
        "\n",
        "    df = None\n",
        "    # Iterate through found tables to identify the S&P 500 constituents table\n",
        "    for df_candidate in tables:\n",
        "        # Check for columns commonly present in the S&P 500 constituents table\n",
        "        # like 'Symbol', 'Ticker', or 'symbol' (case-insensitive)\n",
        "        # Convert column names to string before comparison\n",
        "        cols = [c for c in df_candidate.columns if \"Symbol\" in str(c) or \"Ticker\" in str(c) or \"symbol\" in str(c).lower()]\n",
        "        if cols:\n",
        "            df = df_candidate\n",
        "            break # Found the table\n",
        "\n",
        "    if df is None:\n",
        "        raise ValueError(\"Could not find S&P 500 constituents table on the Wikipedia page.\")\n",
        "\n",
        "    # Ensure 'Symbol' or 'Ticker' column is correctly identified\n",
        "    # Convert column names to string before comparison\n",
        "    col_name = [c for c in df.columns if \"Symbol\" in str(c) or \"Ticker\" in str(c) or \"symbol\" in str(c).lower()][0]\n",
        "\n",
        "    # Clean up ticker symbols (e.g., BRK.B -> BRK-B)\n",
        "    tickers = df[col_name].astype(str).str.replace(\".\", \"-\", regex=False).tolist()\n",
        "    return tickers\n",
        "\n",
        "def download_close_prices(tickers, start, end, batch_size=80):\n",
        "    \"\"\"\n",
        "    Download adjusted close prices in batches to avoid yfinance timeouts.\n",
        "    Returns DataFrame indexed by date with columns=tickers.\n",
        "    \"\"\"\n",
        "    all_close = pd.DataFrame()\n",
        "    tickers_list = list(tickers) # Use a different variable name to avoid confusion with parameter 'tickers'\n",
        "    for i in tqdm(range(0, len(tickers_list), batch_size), desc=\"Downloading prices\"):\n",
        "        batch = tickers_list[i:i+batch_size]\n",
        "        try:\n",
        "            data = yf.download(batch, start=start, end=end, progress=False, group_by='ticker', auto_adjust=True)\n",
        "\n",
        "            if data.empty:\n",
        "                # If yfinance returns an empty DataFrame, skip this batch.\n",
        "                print(f\"Skipping batch {batch}: No data downloaded.\")\n",
        "                continue\n",
        "\n",
        "            close_batch = pd.DataFrame()\n",
        "            # standardized retrieval of 'Close' column\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                # Correctly extract 'Close' prices from a MultiIndex DataFrame\n",
        "                # The first level is ticker, second level is metric (Open, High, Low, Close, Volume)\n",
        "                if 'Close' in data.columns.get_level_values(1):\n",
        "                    close_batch = data.xs('Close', level=1, axis=1)\n",
        "                else:\n",
        "                    print(f\"Skipping batch {batch}: 'Close' prices not found for tickers in batch.\")\n",
        "                    continue\n",
        "            else:\n",
        "                # This case is less common with group_by='ticker', but if it happens,\n",
        "                # check if 'Close' is a direct column.\n",
        "                if 'Close' in data.columns:\n",
        "                    # Assume it's a single ticker's data, structure as a DataFrame\n",
        "                    close_batch = pd.DataFrame({batch[0]: data['Close']})\n",
        "                else:\n",
        "                    print(f\"Skipping batch {batch}: Unexpected data structure without 'Close' column.\")\n",
        "                    continue\n",
        "\n",
        "            all_close = pd.concat([all_close, close_batch], axis=1)\n",
        "        except Exception as e:\n",
        "            print(f\"Batch download error for {batch}: {e}\")\n",
        "    # sort columns to requested order, dropping missing tickers\n",
        "    all_close = all_close.loc[:, all_close.columns.intersection(tickers_list)]\n",
        "    all_close = all_close.sort_index()\n",
        "    return all_close\n",
        "\n",
        "# Fetch tickers and prices\n",
        "sp500_tickers = get_sp500_tickers()\n",
        "universe = [t for t in sp500_tickers]  # copy\n",
        "if BENCH not in universe:\n",
        "    universe.append(BENCH)\n",
        "\n",
        "prices = download_close_prices(universe, start=START_DATE, end=END_DATE)\n",
        "# Remove tickers with insufficient history\n",
        "valid = [t for t in sp500_tickers if prices.get(t, pd.Series()).dropna().shape[0] >= MIN_PRICE_HISTORY_DAYS]\n",
        "print(f\"{len(valid)} tickers with >= {MIN_PRICE_HISTORY_DAYS} days of data.\")"
      ],
      "metadata": {
        "id": "mP4icZnYt9KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature engineering function"
      ],
      "metadata": {
        "id": "mj3HXN8ovDVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_features(close_df, tickers, bench=\"SPY\"):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - close_df: DataFrame of adjusted close prices with dates x tickers\n",
        "      - tickers: list of tickers (subset of close_df.columns)\n",
        "      - bench: benchmark ticker present in close_df\n",
        "    Returns:\n",
        "      - features: DataFrame indexed by ticker (one row per ticker) with required features\n",
        "    \"\"\"\n",
        "    features = pd.DataFrame(index=tickers)\n",
        "    # return windows (trading days approximations)\n",
        "    ret_windows = {\"ret_1m\":21, \"ret_3m\":63, \"ret_6m\":126, \"ret_12m\":252}\n",
        "    for name, days in ret_windows.items():\n",
        "        vals = {}\n",
        "        for t in tickers:\n",
        "            s = close_df[t].dropna()\n",
        "            if s.shape[0] >= days + 1:\n",
        "                vals[t] = (s.iloc[-1] / s.iloc[-days-1]) - 1\n",
        "            else:\n",
        "                vals[t] = np.nan\n",
        "        features[name] = pd.Series(vals)\n",
        "    # vol (rolling 60d annualized)\n",
        "    vol_days = 60\n",
        "    for t in tickers:\n",
        "        s = close_df[t].dropna()\n",
        "        if s.shape[0] >= vol_days + 1:\n",
        "            returns = s.pct_change().dropna()\n",
        "            # compute latest 60-day std of daily returns\n",
        "            if returns.shape[0] >= vol_days:\n",
        "                features.loc[t, \"vol_60d_ann\"] = returns.rolling(vol_days).std().iloc[-1] * math.sqrt(252)\n",
        "            else:\n",
        "                features.loc[t, \"vol_60d_ann\"] = np.nan\n",
        "        else:\n",
        "            features.loc[t, \"vol_60d_ann\"] = np.nan\n",
        "    # beta (1y regression vs bench)\n",
        "    bench_ret = close_df[bench].pct_change().dropna()\n",
        "    for t in tickers:\n",
        "        if t == bench:\n",
        "            features.loc[t, \"beta_1y\"] = 1.0\n",
        "            continue\n",
        "        sret = close_df[t].pct_change().dropna()\n",
        "        common = sret.index.intersection(bench_ret.index)\n",
        "        if len(common) > 200:\n",
        "            Y = sret.loc[common]\n",
        "            X = sm.add_constant(bench_ret.loc[common])\n",
        "            try:\n",
        "                model = sm.OLS(Y, X, missing='drop').fit()\n",
        "                features.loc[t, \"beta_1y\"] = model.params[1]\n",
        "            except Exception:\n",
        "                features.loc[t, \"beta_1y\"] = np.nan\n",
        "        else:\n",
        "            features.loc[t, \"beta_1y\"] = np.nan\n",
        "    # sharpe proxy\n",
        "    for t in tickers:\n",
        "        s = close_df[t].dropna()\n",
        "        if s.shape[0] > 60:\n",
        "            dret = s.pct_change().dropna()\n",
        "            ann_mean = dret.mean() * 252\n",
        "            ann_std = dret.std() * math.sqrt(252)\n",
        "            features.loc[t, \"sharpe_1y\"] = ann_mean / (ann_std + 1e-9)\n",
        "        else:\n",
        "            features.loc[t, \"sharpe_1y\"] = np.nan\n",
        "    # momentum (blend 6m & 3m)\n",
        "    features[\"momentum\"] = 0.6 * features[\"ret_6m\"] + 0.4 * features[\"ret_3m\"]\n",
        "    # add simple fundamentals via yfinance info (best-effort; may be slow)\n",
        "    fund_keys = [\"marketCap\", \"trailingPE\", \"priceToBook\", \"earningsQuarterlyGrowth\", \"revenueQuarterlyGrowth\"]\n",
        "    fund_df = pd.DataFrame(index=tickers, columns=fund_keys)\n",
        "    for t in tqdm(tickers, desc=\"Fetching fundamentals (yfinance)\"):\n",
        "        try:\n",
        "            info = yf.Ticker(t).info\n",
        "            for k in fund_keys:\n",
        "                fund_df.loc[t, k] = info.get(k, np.nan)\n",
        "        except Exception:\n",
        "            for k in fund_keys:\n",
        "                fund_df.loc[t, k] = np.nan\n",
        "    # join and do log transform for marketCap\n",
        "    features = features.join(fund_df)\n",
        "    features[\"log_mktcap\"] = np.log(features[\"marketCap\"].replace(0, np.nan).astype(float)) # Convert to float\n",
        "    # Cleaning: drop tickers with too many NaNs, then median impute\n",
        "    features = features.dropna(thresh=int(features.shape[1]*0.55))  # keep tickers with at least 55% non-null\n",
        "    # Drop columns that are entirely NaN *after* row dropping, before median imputation\n",
        "    # Otherwise, features.median() for such columns will be NaN, and fillna will do nothing.\n",
        "    features = features.dropna(axis=1, how='all')\n",
        "    features = features.fillna(features.median())\n",
        "    return features\n",
        "\n",
        "# compute features\n",
        "features = compute_features(prices, valid, bench=BENCH)\n",
        "features.shape"
      ],
      "metadata": {
        "id": "-PaY5tHkvgGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Clustering helper: run KMeans for k range and choose best k by silhouette"
      ],
      "metadata": {
        "id": "x_PNHnN2vhCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_kmeans_for_range(X_df, ks=range(2,11), random_state=RNG):\n",
        "    # Ensure no NaNs are passed to KMeans\n",
        "    X_df = X_df.dropna()\n",
        "    if X_df.empty:\n",
        "        return {}, None # Return empty results if no valid data remains\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = pd.DataFrame(scaler.fit_transform(X_df), index=X_df.index, columns=X_df.columns)\n",
        "    results = {}\n",
        "    for k in ks:\n",
        "        km = KMeans(n_clusters=k, n_init=30, random_state=random_state)\n",
        "        labels = km.fit_predict(X_scaled)\n",
        "        sil = silhouette_score(X_scaled, labels)\n",
        "        ch = calinski_harabasz_score(X_scaled, labels)\n",
        "        results[k] = {\"model\": km, \"labels\": labels, \"silhouette\": sil, \"calinski\": ch, \"inertia\": km.inertia_}\n",
        "    return results, X_scaled\n",
        "\n",
        "# Example run (for full pipeline we'll call inside rebalance loop)\n",
        "km_results, X_scaled = run_kmeans_for_range(features.select_dtypes(include=[np.number]), ks=KS)\n",
        "\n",
        "# Check if km_results is empty before processing\n",
        "if km_results:\n",
        "    scores_df = pd.DataFrame({k: {\"silhouette\": km_results[k][\"silhouette\"], \"calinski\": km_results[k][\"calinski\"], \"inertia\": km_results[k][\"inertia\"]} for k in km_results}).T\n",
        "    print(scores_df)\n",
        "    best_k = max(km_results.keys(), key=lambda kk: km_results[kk][\"silhouette\"])\n",
        "    print(f\"Best k based on silhouette score: {best_k}\")\n",
        "else:\n",
        "    print(\"No valid data to perform clustering. km_results is empty.\")\n",
        "    best_k = None # or handle appropriately if best_k is used later\n"
      ],
      "metadata": {
        "id": "04NZLz3Evkbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA on features and plot the clusters with best k\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Perform PCA on scaled feature matrix\n",
        "scaler = StandardScaler()\n",
        "df_features_scaled = pd.DataFrame(scaler.fit_transform(features), index=features.index, columns=features.columns)\n",
        "pca = PCA(n_components=2)\n",
        "pca_components = pca.fit_transform(df_features_scaled)\n",
        "\n",
        "# Assign cluster labels from the best_k result\n",
        "cluster_labels = km_results[best_k][\"labels\"]\n",
        "\n",
        "# Create a PCA result dataframe\n",
        "pca_df = pd.DataFrame({\n",
        "    'PCA1': pca_components[:, 0],\n",
        "    'PCA2': pca_components[:, 1],\n",
        "    'Cluster': cluster_labels,\n",
        "    'Ticker': features.index\n",
        "})\n",
        "\n",
        "# Variance explained\n",
        "expl_var = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratio:\", expl_var)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for cluster in sorted(pca_df['Cluster'].unique()):\n",
        "    subset = pca_df[pca_df['Cluster'] == cluster]\n",
        "    plt.scatter(subset['PCA1'], subset['PCA2'], s=60, alpha=0.7, label=f'Cluster {cluster}')\n",
        "\n",
        "# Label a few major stocks to improve readability\n",
        "for _, row in pca_df.sample(min(20, len(pca_df))).iterrows():\n",
        "    plt.text(row['PCA1'], row['PCA2'], row['Ticker'], fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.title(\"PCA Projection of Stock Clusters\")\n",
        "plt.xlabel(f\"PCA1 ({expl_var[0]*100:.1f}% variance)\")\n",
        "plt.ylabel(f\"PCA2 ({expl_var[1]*100:.1f}% variance)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zvb2_HLE5-tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Choose best cluster by balanced risk + momentum"
      ],
      "metadata": {
        "id": "XgpOJa4yvnDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "def choose_best_cluster(features_df, labels):\n",
        "    \"\"\"\n",
        "    Compute cluster-level mean metrics and composite score:\n",
        "      score = z(sharpe_1y) - z(vol_60d_ann) + z(momentum)\n",
        "    Returns best_cluster (int) and cluster_stats DataFrame\n",
        "    \"\"\"\n",
        "    cluster_series = pd.Series(labels, index=features_df.index, name=\"cluster\")\n",
        "    df = features_df.join(cluster_series)\n",
        "    agg = df.groupby(\"cluster\").agg({\n",
        "        \"sharpe_1y\": \"mean\",\n",
        "        \"vol_60d_ann\": \"mean\",\n",
        "        \"momentum\": \"mean\",\n",
        "        \"ret_12m\": \"mean\"\n",
        "    }).rename(columns={\"ret_12m\":\"avg_12m_ret\"})\n",
        "    # zscore, with handling constant columns\n",
        "    agg_z = agg.apply(lambda col: zscore(col) if col.std() != 0 else (col - col.mean()))\n",
        "    agg[\"score\"] = agg_z[\"sharpe_1y\"] - agg_z[\"vol_60d_ann\"] + agg_z[\"momentum\"]\n",
        "    agg = agg.sort_values(\"score\", ascending=False)\n",
        "    return int(agg.index[0]), agg\n",
        "\n",
        "# quick example with best_k\n",
        "labels = km_results[best_k][\"labels\"]\n",
        "best_cluster, cluster_stats = choose_best_cluster(features, labels)\n",
        "best_cluster, cluster_stats.head()"
      ],
      "metadata": {
        "id": "Zbv32siwvqYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Momentum-weighted allocation inside cluster (with optional cap)"
      ],
      "metadata": {
        "id": "qh0ic8jgv1oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_weights(features_df, tickers, momentum_col=\"momentum\", max_weight=None):\n",
        "    \"\"\"\n",
        "    Produce positive-only momentum weights in tickers list.\n",
        "    If all momentums <=0, fallback to 12m returns.\n",
        "    Apply max_weight cap (if provided) and renormalize.\n",
        "    \"\"\"\n",
        "    m = features_df.loc[tickers, momentum_col].copy()\n",
        "    if (m <= 0).all():\n",
        "        # fallback to ret_12m\n",
        "        m = features_df.loc[tickers, \"ret_12m\"].copy()\n",
        "    m_pos = m.clip(lower=0) + 1e-9\n",
        "    raw = m_pos / m_pos.sum()\n",
        "    if max_weight is not None:\n",
        "        clipped = raw.clip(upper=max_weight)\n",
        "        if clipped.sum() == 0:\n",
        "            # if weird numerical case, revert to raw\n",
        "            weights = raw\n",
        "        else:\n",
        "            weights = clipped / clipped.sum()\n",
        "    else:\n",
        "        weights = raw\n",
        "    return weights.sort_values(ascending=False)\n",
        "\n",
        "# example\n",
        "example_weights = momentum_weights(features, [t for t in features.index[:10]], max_weight=MAX_WEIGHT)\n",
        "example_weights.head()"
      ],
      "metadata": {
        "id": "M01skfZEv9v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Walk-forward backtest (monthly rebalancing)"
      ],
      "metadata": {
        "id": "pPh_Xx_JwBbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def walkforward_backtest(close_prices, features_all, bench=\"SPY\", rebalance_freq=\"M\", ks=range(2,11),\n",
        "                         lookback=252*1, max_weight=0.10, trans_cost_bps=5):\n",
        "    \"\"\"\n",
        "    close_prices: DataFrame date x tickers (must include bench)\n",
        "    features_all: precomputed features DataFrame indexed by tickers (useable columns)\n",
        "    rebalance_freq: pandas offset alias (e.g., 'M' monthly)\n",
        "    ks: k range for KMeans\n",
        "    lookback: days used for features (not strictly enforced here; we use features_all which assumes computed)\n",
        "    returns: DataFrame of portfolio daily returns, weights history, and diagnostics\n",
        "    \"\"\"\n",
        "    # Build rebalancing calendar based on available price dates\n",
        "    dates = close_prices.index\n",
        "    # We'll pick the last trading day of each calendar month present in dates\n",
        "    rebal_dates = dates.to_series().resample(rebalance_freq).last().dropna()\n",
        "    rebal_dates = rebal_dates[rebal_dates >= (dates[0] + pd.Timedelta(days=60))]  # ensure some history\n",
        "    rebal_dates = rebal_dates[rebal_dates <= dates[-1]]\n",
        "    rebal_dates = list(rebal_dates)\n",
        "\n",
        "    # containers\n",
        "    daily_portfolio_ret = pd.Series(index=dates, dtype=float).fillna(0.0)\n",
        "    weights_history = {}\n",
        "    turnover_history = pd.DataFrame(index=rebal_dates, columns=[\"turnover\", \"transaction_cost\"])\n",
        "    prev_weights = pd.Series(dtype=float)\n",
        "\n",
        "    for i, rb_date in enumerate(tqdm(rebal_dates, desc=\"Rebalance loop\")):\n",
        "        # Use information up to rb_date (we already computed features_all from full history; in production recompute features using only up-to-date data)\n",
        "        # Subset tickers available at rb_date\n",
        "        available = [t for t in features_all.index if (t in close_prices.columns) and (not np.isnan(close_prices.loc[:rb_date, t]).all())]\n",
        "        if len(available) < 10:\n",
        "            continue\n",
        "\n",
        "        # prepare features subset and run clustering on numeric features\n",
        "        fsub = features_all.loc[available].select_dtypes(include=[np.number])\n",
        "        km_results, X_scaled = run_kmeans_for_range(fsub, ks=ks, random_state=RNG)\n",
        "        best_k = max(km_results.keys(), key=lambda kk: km_results[kk][\"silhouette\"])\n",
        "        labels = km_results[best_k][\"labels\"]\n",
        "        best_cluster, cluster_stats = choose_best_cluster(fsub, labels)\n",
        "        # select tickers in best cluster\n",
        "        cluster_series = pd.Series(labels, index=fsub.index, name=\"cluster\")\n",
        "        selected = cluster_series[cluster_series == best_cluster].index.tolist()\n",
        "        if len(selected) == 0:\n",
        "            continue\n",
        "\n",
        "        # compute momentum weights and apply cap\n",
        "        w = momentum_weights(features_all, selected, momentum_col=\"momentum\", max_weight=max_weight)\n",
        "        # store weights for this rebalance date\n",
        "        weights_history[rb_date] = w\n",
        "\n",
        "        # compute daily returns from rb_date (inclusive) to next rebalance date (exclusive)\n",
        "        start_idx = close_prices.index.get_loc(rb_date)\n",
        "        if i+1 < len(rebal_dates):\n",
        "            next_rb = rebal_dates[i+1]\n",
        "            end_idx = close_prices.index.get_loc(next_rb)  # exclusive\n",
        "            period_dates = close_prices.index[start_idx:end_idx]\n",
        "        else:\n",
        "            period_dates = close_prices.index[start_idx:]\n",
        "        # compute daily returns for selected tickers\n",
        "        sub = close_prices[selected].loc[period_dates]\n",
        "        ret = sub.pct_change().fillna(0)\n",
        "        # apply portfolio weights: for first day, use weights w; assume static until next rebalance\n",
        "        # portfolio daily return = sum(weights * tickers daily return)\n",
        "        weights_array = w.reindex(selected).fillna(0).values\n",
        "        p_ret = (ret.fillna(0) * weights_array).sum(axis=1)\n",
        "        daily_portfolio_ret.loc[period_dates] = p_ret.values\n",
        "\n",
        "        # turnover: compare prev_weights to new w on the rebalance date\n",
        "        if prev_weights.size > 0:\n",
        "            # align prev and current, fill zeros\n",
        "            prev = prev_weights.reindex(w.index).fillna(0)\n",
        "            curr = w.reindex(prev.index).fillna(0)\n",
        "            turnover = (prev - curr).abs().sum() / 2.0\n",
        "        else:\n",
        "            turnover = w.abs().sum()  # initial full buy\n",
        "        transaction_cost = turnover * (trans_cost_bps/10000.0)\n",
        "        turnover_history.loc[rb_date] = [turnover, transaction_cost]\n",
        "        prev_weights = w.copy()\n",
        "\n",
        "    # create portfolio returns series\n",
        "    port_ret = daily_portfolio_ret.fillna(0)\n",
        "    bench_ret = close_prices[bench].pct_change().fillna(0).reindex(port_ret.index).fillna(0)\n",
        "\n",
        "    # subtract transaction cost at rebalance days (apply as a negative return on the rebalance date)\n",
        "    for d, row in turnover_history.dropna().iterrows():\n",
        "        tc = float(row[\"transaction_cost\"])\n",
        "        if d in port_ret.index:\n",
        "            port_ret.loc[d] = port_ret.loc[d] - tc\n",
        "\n",
        "    # aggregate outputs\n",
        "    weights_df = pd.DataFrame({d: weights_history[d] for d in weights_history}).T.fillna(0).sort_index()\n",
        "    return port_ret, bench_ret, weights_df, turnover_history\n",
        "\n",
        "# Run the walkforward backtest (this may take several minutes due to yfinance fundamentals + clustering per rebalance)\n",
        "port_ret, bench_ret, weights_df, turnover_history = walkforward_backtest(prices, features, bench=BENCH,\n",
        "                                                                         rebalance_freq=REBALANCE_FREQ, ks=KS,\n",
        "                                                                         lookback=252*1, max_weight=MAX_WEIGHT,\n",
        "                                                                         trans_cost_bps=TRANSACTION_COST_BPS)\n"
      ],
      "metadata": {
        "id": "J_5YDamvwFPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Performance metrics & helper functions"
      ],
      "metadata": {
        "id": "FD2PYgNSwLz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def performance_table(returns, freq=252):\n",
        "    \"\"\"\n",
        "    returns: pd.Series of daily returns\n",
        "    freq: trading days per year\n",
        "    \"\"\"\n",
        "    ann_ret = (1 + returns).prod() ** (freq / len(returns)) - 1\n",
        "    ann_vol = returns.std() * math.sqrt(freq)\n",
        "    sharpe = ann_ret / (ann_vol + 1e-9)\n",
        "    cum = (1 + returns).cumprod() - 1\n",
        "    peak = cum.cummax()\n",
        "    drawdown = (cum - peak)\n",
        "    max_drawdown = drawdown.min()\n",
        "    return {\"CAGR\": ann_ret, \"AnnVol\": ann_vol, \"Sharpe\": sharpe, \"MaxDrawdown\": max_drawdown, \"TotalReturn\": cum.iloc[-1]}\n",
        "\n",
        "def compute_drawdown(returns):\n",
        "    cum = (1 + returns).cumprod()\n",
        "    peak = cum.cummax()\n",
        "    dd = (cum / peak) - 1\n",
        "    return dd\n",
        "\n",
        "# compute metrics\n",
        "perf_port = performance_table(port_ret.dropna())\n",
        "perf_bench = performance_table(bench_ret.dropna())\n",
        "\n",
        "print(\"Portfolio performance:\", perf_port)\n",
        "print(\"Benchmark performance:\", perf_bench)\n"
      ],
      "metadata": {
        "id": "PS2g0VJLwPEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Charts: cumulative returns, drawdown, rolling volatility"
      ],
      "metadata": {
        "id": "EHgOl1r1wQsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cumulative returns\n",
        "cum_port = (1 + port_ret).cumprod()\n",
        "cum_bench = (1 + bench_ret).cumprod()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(cum_port.index, cum_port.values)\n",
        "plt.plot(cum_bench.index, cum_bench.values)\n",
        "plt.title(\"Cumulative Returns: Strategy vs SPY\")\n",
        "plt.legend([\"Strategy\", \"SPY\"])\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# drawdown\n",
        "dd_port = compute_drawdown(port_ret)\n",
        "dd_bench = compute_drawdown(bench_ret)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(dd_port.index, dd_port.values)\n",
        "plt.plot(dd_bench.index, dd_bench.values)\n",
        "plt.title(\"Drawdown (fractional) - Strategy vs SPY\")\n",
        "plt.legend([\"Strategy DD\", \"SPY DD\"])\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# rolling annualized volatility (60-day)\n",
        "roll_vol_port = port_ret.rolling(window=60).std() * math.sqrt(252)\n",
        "roll_vol_bench = bench_ret.rolling(window=60).std() * math.sqrt(252)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(roll_vol_port.index, roll_vol_port.values)\n",
        "plt.plot(roll_vol_bench.index, roll_vol_bench.values)\n",
        "plt.title(\"Rolling 60-day Annualized Volatility\")\n",
        "plt.legend([\"Strategy\", \"SPY\"])\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jULRLxgzwUiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Inspect weights & turnover"
      ],
      "metadata": {
        "id": "O6yA1X9owcn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the most recent weights\n",
        "if not weights_df.empty:\n",
        "    latest_weights = weights_df.iloc[-1].sort_values(ascending=False).head(20)\n",
        "    print(\"Latest top weights:\")\n",
        "    display(latest_weights)\n",
        "else:\n",
        "    print(\"No weights computed.\")\n",
        "\n",
        "# Plot number of holdings per rebalance\n",
        "num_holdings = (weights_df > 0).sum(axis=1)\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(num_holdings.index, num_holdings.values)\n",
        "plt.title(\"Number of Holdings per Rebalance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Turnover summary\n",
        "if not turnover_history.dropna().empty:\n",
        "    display(turnover_history.dropna().head(10))\n"
      ],
      "metadata": {
        "id": "JRWzYFN0wehv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}